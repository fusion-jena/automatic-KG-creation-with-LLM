 The performance of the deep learning model is evaluated using the COCO evaluation metric, which
determines whether the predicted organs and their locations are correct. The minimum threshold
chosen for any prediction to be acceptable is having a confidence score (probability) of 0.5. The
COCO method calculates average precision (with values from 0 to 100), which is a metric that
encapsulates both precision and recall of the detection, for the entire predictions and each class
of organs at different levels of Intersection over Union (IoU). IoU is an evaluation metric that
quantifies the overlap of the predicted bounding boxes with the ground-truth bounding boxes. The IoU
score ranges from 0 to 1, the higher the overlap, the higher the IoU score. The evaluation method
considers all predictions as positive that have IoU of at least 0.5 and the average precision at
this level of IoU is called AP50. Similarly, the average precision with a minimum IoU of 0.75 is
called AP75, whereas AP is the average over 10 IoU levels from 0.5 to 0.95 with a step size of 0.05.
The precision metrics evaluated on the predicted organs on the test subset are shown in Table 2. The
COCO method also calculates the AP for each category, as shown in Table 3, along with the total
bounding boxes for each category in the test subset.