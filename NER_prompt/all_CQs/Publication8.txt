 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ1: What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)?
Answer: The context does not provide information on the methods utilized for collecting raw data in the deep learning pipeline.

CQ2: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?
Answer: The data format used in the deep learning pipeline is image data. The deep learning pipeline in this study uses image data for detecting plant organs on herbarium scans. The plant organs are detected using an object detection network, which works by localising each object with a bounding box on the image and classifying it. The network used in this study is called Faster R-CNN, which is based on Convolutional Neural Networks (CNN) and is used for image data. Therefore, the data format used in the deep learning pipeline is image data.

CQ3: What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?
Answer: The data annotation technique used in the deep learning pipeline for detecting plant organs on herbarium scans is bounding box annotation. This is used to localise each object with a bounding box on the image and classify it.

CQ4: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?
Answer: The paper does not provide information on the specific data augmentation techniques applied in the deep learning pipeline.

CQ5: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)?
Answer: The deep learning pipeline in this study uses the ImageNet dataset for transfer learning. The initial layers of the CNN are initialized with pre-trained weights from the ImageNet dataset, and the deeper layers are fine-tuned using the annotated herbarium scan dataset.

CQ6: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: Before training a deep learning model, several preprocessing steps are often involved. These can include normalization, scaling, and cleaning. Normalization is a technique used to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values or losing information. Scaling is a method used to standardize the range of independent variables or features of data. Cleaning is the process of removing or correcting erroneous data from a dataset. However, the specific preprocessing steps can vary depending on the dataset and the model being used. In this study, the herbarium scans were rescaled from their original average size of ca. 5100 by 3500 pixels to 1200 by 800 pixels, in order to preserve the aspect ratio of the scans and to speed up the learning by reducing the number of pixels.

CQ7: What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?
Answer: The data was split into a training subset of 498 images and a test subset of 155 images. The model was trained on the training subset and evaluated on the test subset. The model was then trained again on all 653 annotated images for predicting plant organs on another un-annotated independent dataset to evaluate the method. This dataset consists of 708 full scale herbarium scans from the Herbarium Senckenbergianum (FR) with a different set of species and geographical origins.

CQ8: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?
Answer: The Python code and the trained model have been made available at GitHub (Younis 2020).

CQ9: Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?
Answer: The data repository of the deep learning pipeline is not explicitly mentioned in the provided context. Therefore, I cannot provide a specific answer to this question.

CQ10: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: I'm sorry, the provided context does not include information about the code repository link of the deep learning pipeline.

CQ11: What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?
Answer: I'm sorry, but the provided context does not mention a specific data repository link for the deep learning pipeline.

CQ12: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The type of deep learning model used in the pipeline is a Convolutional Neural Network (CNN). Specifically, the Faster R-CNN model is used, which is based on the ResNet-50 architecture.

CQ13: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The deep learning model was implemented with the Detectron2 library in PyTorch framework and trained using Stochastic Gradient Descent optimiser with a learning rate of 0. 0025 and momentum of 0. 9. The anchor generator in the Region Proposal Network had six anchor scales [32, 64, 128, 256, 512, 1024] (square root of area in absolute pixels) each with three aspect ratios of [1:2, 1:1, 2:1]. The thresholds for non-maximum suppression (NMS) were 0. 6 for training and 0. 25 for testing, respectively.

CQ14: How are the hyperparameters of the model optimized (e.g., grid search, random search)?
Answer: The hyperparameters of the model were optimized using Stochastic Gradient Descent optimizer with a learning rate of 0. 0025 and momentum of 0. 9. The anchor generator in the Region Proposal Network had six anchor scales [32, 64, 128, 256, 512, 1024] (square root of area in absolute pixels) each with three aspect ratios of [1:2, 1:1, 2:1]. The thresholds for non-maximum suppression (NMS) were 0. 6 for training and 0. 25 for testing, respectively.

CQ15: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The model was implemented with the Detectron2 library in PyTorch framework and trained using Stochastic Gradient Descent optimizer with a learning rate of 0. 0025 and momentum of 0. 9.

CQ16: What criteria are used to determine when training is complete (e.g., validation loss plateau)?
Answer: The model was trained twice, once with a training subset of 498 images on a single GPU for 9000 iterations and performance evaluated on the test subset of 155 images, also on a single GPU and then trained again on all 653 annotated images on three GPUs for 18000 iterations for predicting plant organs on another un-annotated independent dataset to evaluate our method. This dataset consists of 708 full scale herbarium scans, with an average size of ca. 9600 by 6500 pixels, from the Herbarium Senckenbergianum (FR) (Otte et al. 2011) with a different set of species (Fig. 2) and geographical origins, which is also available at GBIF (Senckenberg 2020). The Python code and the trained model have been made available at GitHub (Younis 2020). The criteria used to determine when training is complete are not explicitly mentioned in the provided context. However, it can be inferred that the training was completed after a certain number of iterations, specifically 9000 iterations for the first training and 18000 iterations for the second training. The performance was then evaluated on the test subset and the un-annotated independent dataset, respectively.

CQ17: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The paper does not provide information on the regularization methods used to prevent overfitting in the deep learning pipeline.

CQ18: What is the strategy implemented to monitor the model performance during training?
Answer: The strategy implemented to monitor the model performance during training is not explicitly mentioned in the provided context. However, it is mentioned that the model was trained using Stochastic Gradient Descent optimiser with a learning rate of 0. 0025 and momentum of 0. 9. This suggests that the model performance was likely monitored using metrics such as loss and accuracy during training.

CQ19: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The deep learning model is implemented with the Detectron2 library in the PyTorch framework.

CQ20: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The model was trained using TITAN Xp GPUs. Specifically, a minibatch size of four images per GPU was selected for training the model. The model was trained twice, once with a training subset of 498 images on a single GPU for 9000 iterations and then trained again on all 653 annotated images on three GPUs for 18000 iterations.

CQ21: What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?
Answer: The postprocessing steps involved after the model training are not explicitly mentioned in the provided context. However, based on the context, we can infer that the model performance was evaluated using visual inspection and the Average Precision (AP) metric. The Average Precision metric was calculated for each type of organ and for the entire dataset. Additionally, the model was tested on a different un-annotated dataset, and the predicted bounding boxes were manually annotated to evaluate the model's performance. The context does not mention the use of saliency maps or confusion matrix for postprocessing.

CQ22: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The performance of the deep learning model is evaluated using the COCO evaluation metric, which determines whether the predicted organs and their locations are correct. The minimum threshold chosen for any prediction to be acceptable is having a confidence score (probability) of 0. 5. The COCO method calculates average precision (with values from 0 to 100), which is a metric that encapsulates both precision and recall of the detection, for the entire predictions and each class of organs at different levels of Intersection over Union (IoU). IoU is an evaluation metric that quantifies the overlap of the predicted bounding boxes with the ground-truth bounding boxes. The IoU score ranges from 0 to 1, the higher the overlap, the higher the IoU score. The evaluation method considers all predictions as positive that have IoU of at least 0. 5 and the average precision at this level of IoU is called AP50. Similarly, the average precision with a minimum IoU of 0. 75 is called AP75, whereas AP is the average over 10 IoU levels from 0. 5 to 0. 95 with a step size of 0. 05. The precision metrics evaluated on the predicted organs on the test subset are shown in Table 2. The COCO method also calculates the AP for each category, as shown in Table 3, along with the total bounding boxes for each category in the test subset.

CQ23: What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?
Answer: The generalizability of the deep learning model was ensured by training it on a diverse dataset and evaluating its performance on an independent dataset. The model was trained twice, first on a training subset of 498 images and then on all 653 annotated images. The performance of the model was evaluated on a test subset of 155 images and then on another un-annotated independent dataset consisting of 708 full scale herbarium scans from the Herbarium Senckenbergianum (FR) with a different set of species and geographical origins. This dataset is also available at GBIF (Senckenberg 2020).

CQ24: What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?
Answer: The context does not provide information on strategies to handle randomness in the deep learning pipeline, such as managing random seed values.

CQ25: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is object detection and localisation. It is used to detect and localise each object with a bounding box on the image and classify it. The model used in this study is called Faster R-CNN.

CQ26: What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?
Answer: The paper does not provide information on techniques used to address data bias during preprocessing of the deep learning pipeline.

CQ27: What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?
Answer: The trained deep learning model was deployed by implementing it with the Detectron2 library in the PyTorch framework. The model was trained using Stochastic Gradient Descent optimiser with a learning rate of 0. 0025 and momentum of 0. 9. The anchor generator in the Region Proposal Network had six anchor scales [32, 64, 128, 256, 512, 1024] (square root of area in absolute pixels) each with three aspect ratios of [1:2, 1:1, 2:1]. The thresholds for non-maximum suppression (NMS) were 0. 6 for training and 0. 25 for testing, respectively. Due to the large image size and additional parameters of Faster R-CNN, a minibatch size of four images per GPU (TITAN Xp) was selected for training the model. The model was trained twice, once with a training subset of 498 images on a single GPU for 9000 iterations and performance evaluated on the test subset of 155 images, also on a single GPU and then trained again on all 653 annotated images on three GPUs for 18000 iterations for predicting plant organs on another un-annotated independent dataset to evaluate our method. This dataset consists of 708 full scale herbarium scans, with an average size of ca. 9600 by 6500 pixels, from the Herbarium Senckenbergianum (FR) (Otte et al. 2011) with a different set of species (Fig. 2) and geographical origins, which is also available at GBIF (Senckenberg 2020). The Python code and the trained model have been made available at GitHub (Younis 2020).

CQ28: Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?
Answer: The context does not provide information on the platform used to deploy the deep learning model.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ... 