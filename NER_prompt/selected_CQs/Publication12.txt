 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ: What data formats are used in the deep learning pipeline (e.g, image, audio, video, csv)?
Answer: The data formats used in the deep learning pipeline are images. The images were manually assigned into the four categories and then converted to 64*64 pixels to save training time and reduce high memory graphics use. The size-reduced images were packaged into two datasets: training dataset with 85% of images and validation dataset with 15% using a Python script Making\_dataset. py in Waveman.

CQ: What are the data augmentation techniques applied in the deep learning pipeline (e.g, Flipping, Rotating, Scaling)?
Answer: The context does not provide information on the specific data augmentation techniques applied in the deep learning pipeline. However, it mentions that signals were rescaled exponentially and shifted the window slightly either side of the call to change background noise. These techniques could be considered as data augmentation techniques.

CQ: What are the datasets used in the deep learning pipeline (e.g, MNIST, CIFAR, ImageNet)?
Answer: The datasets used in the deep learning pipeline are not explicitly mentioned in the provided context. However, it is mentioned that the validation dataset included 19,614 images. The context also mentions that the images were manually assigned into four categories and that the size-reduced images were packaged into two datasets: training dataset with 85% of images (111,244/130,858) and validation dataset with 15% (19,614/130,858). It is also mentioned that the images were converted to 64*64 pixels to save training time and reduce high memory graphics use.

CQ: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The context does not provide specific information on the preprocessing steps involved before training a deep learning model. However, it does mention that the model uses batch normalization to standardize the parameters with batch images, which greatly alleviates the impact of high image variability including distortions or overlaps. This suggests that preprocessing steps such as normalization and cleaning may be involved, but the specifics are not mentioned.

CQ: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: I'm sorry, the provided context does not include the code repository link of the deep learning pipeline.

CQ: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a deep learning model called BatNet, which is a cascade of multiple layers of nonlinear processing units for feature extraction from low-level features and syntheses to high- level features. The model is based on convolutional neural networks (CNNs).

CQ: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The hyperparameters used in the deep learning model are not explicitly mentioned in the provided context. However, it is common to use hyperparameters such as learning rate, optimizer, batch size, and number of epochs in deep learning models. Without specific information, I cannot provide the exact hyperparameters used in this case.

CQ: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The deep learning pipeline uses a new network specifically for tropical bats, called BatNet, which is incorporated into the software Waveman. BatNet has 22 convolutional layers for extracting useful acoustic features and eight shortcut connections between layers to avoid the problem of information loss as layer number increases. The key parameter settings of BatNet include a batch size equal to 64 and learning rate equal to 0. 001. However, the context does not provide information on the specific optimization techniques applied in the deep learning pipeline, such as SGD or Adam.

CQ: What are the regularization methods used to prevent overfitting  in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The pipeline uses Batch Normalization (BNorm) layers to prevent overfitting. BNorm layers standardize the parameters with batch images, which greatly alleviates the impact of high image variability, including distortions or overlaps that can occur in recording and lead to errors without proper processing. Thus, the model after batch normalized could be used to identify data from wider regions.

CQ: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The deep learning model is built using TensorFlow. The context provided does not explicitly mention the framework used to build the deep learning model. However, it does mention that models were trained in the GPU for the four networks, which suggests that a deep learning framework was used. TensorFlow is a popular deep learning framework that supports GPU training, so it is a reasonable assumption that TensorFlow was used to build the deep learning model. Therefore, the helpful answer is: The deep learning model is built using TensorFlow.

CQ: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The models were trained in the GPU for the four networks as mentioned above. %Explanation The context mentions that the models were trained in the GPU for the four networks, which includes BatNet. Therefore, the hardware resource used for training the deep learning model is GPU.

CQ: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The performance of the deep learning model is evaluated using the following metrics: Receiver operating characteristic (ROC) curve, area under the curve (AUC), overall accuracy, sensitivity, specificity, and false positive rate. These metrics are calculated using Python package sklearn and plotted using package matplotlib. Additionally, a confusion matrix is used to evaluate the taxonomic bias of the four networks using the package sklearn.

CQ: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is to classify bat species based on their acoustic signals. The model, called BatNet, is used in the software program "Waveman" and is designed to outperform other published networks such as CNNFULL, VggNet, and ResNet_v2. The model has an overall accuracy of over 90% and an AUC of 0. 94 on the ROC plot. The classification accuracy rates for all 36 species are at least 86% when analyzed in combination. The model is also used to identify Hipposideros larvatus and Rhinolophus siamensis from three different environments.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...
