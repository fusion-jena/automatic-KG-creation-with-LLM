 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ: What data formats are used in the deep learning pipeline (e.g, image, audio, video, csv)?
Answer: The deep learning pipeline in this study uses MODIS-derived NDVI and LAI images as inputs. Therefore, the data format used in this deep learning pipeline is image data.

CQ: What are the data augmentation techniques applied in the deep learning pipeline (e.g, Flipping, Rotating, Scaling)?
Answer: The context does not provide information on the data augmentation techniques applied in the deep learning pipeline. Therefore, I cannot provide an answer to this query.

CQ: What are the datasets used in the deep learning pipeline (e.g, MNIST, CIFAR, ImageNet)?
Answer: The datasets used in the deep learning pipeline are not explicitly mentioned in the provided context. However, it is mentioned that a time-series of MODIS-driven NDVI and LAI were used in the deep learning-based species richness modeling.

CQ: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: Before training a deep learning model, several preprocessing steps are typically involved. These can include normalization, which scales the input data to a specific range, often between 0 and 1 or -1 and 1. Scaling is another common step, which can involve techniques like standardization or min- max scaling to ensure that all features have a similar scale. Cleaning the data is also important, which can involve handling missing values, removing outliers, or dealing with inconsistent data. Other preprocessing steps can include encoding categorical variables, feature selection, or data augmentation. The specific preprocessing steps used can depend on the nature of the data and the requirements of the deep learning model.

CQ: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: I'm sorry, the provided context does not include information about the code repository link of the deep learning pipeline.

CQ: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The type of deep learning model used in the pipeline is a multilayer perceptron (MLP).

CQ: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The context does not provide information on the hyperparameters used in the deep learning model, such as the learning rate or optimizer.

CQ: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The context does not provide information on the optimization techniques applied in the deep learning pipeline.

CQ: What are the regularization methods used to prevent overfitting  in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The regularization methods used to prevent overfitting in the deep learning pipeline include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. L2 regularization, also known as weight decay, is a method that adds a penalty term to the loss function, which discourages large weights and encourages smaller, more generalized models. %Query

CQ: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The context does not provide information on the specific deep learning frameworks used to build the model.

CQ: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The context does not provide information on the hardware resources used for training the deep learning model.

CQ: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The root mean square error (RMSE) is used as the performance metric to evaluate the deep learning model.

CQ: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is to develop a species richness retrieval model by integration of the surveyed and estimated richness information collected in South Korea and multitemporal remote sensing data. The purpose of the deep learning model in this study is to develop a species richness retrieval model by integrating the surveyed and estimated richness information collected in South Korea and multitemporal remote sensing data.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...
