 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ: What data formats are used in the deep learning pipeline (e.g, image, audio, video, csv)?
Answer: The data formats used in the deep learning pipeline are grey-coloured 1-channel images, specifically audio spectrograms. These images are fed to the input-layer of the U-net. The image fed to the output layer during training is the segmentation of the spectrogram of the input image.

CQ: What are the data augmentation techniques applied in the deep learning pipeline (e.g, Flipping, Rotating, Scaling)?
Answer: The data augmentation techniques applied in the deep learning pipeline are small random rotations between -5 and 5 degrees as well as random displacements at the maximum of the 10% of the image. Although this is a large displacement to apply in order to accommodate within-species vocal variations, it was decided to use it and proved beneficial probably because a global bird detector was made and not one dedicated to a specific species.

CQ: What are the datasets used in the deep learning pipeline (e.g, MNIST, CIFAR, ImageNet)?
Answer: The context does not provide specific information about the datasets used in the deep learning pipeline. However, it mentions that the training set is composed of spectrogram figures of bird recordings as well as recordings void of any bird activity and their corresponding binary masks.

CQ: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The context does not provide specific information on the preprocessing steps involved before training the deep learning model. However, common preprocessing steps in deep learning include normalization, scaling, cleaning, and data augmentation. These steps help to improve the model's performance and convergence during training.

CQ: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: The code repository link of the deep learning pipeline is not provided in the given context.

CQ: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a U-net deep autoencoder, which is a type of convolutional neural network (CNN).

CQ: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The paper does not provide information on the hyperparameters used in the deep learning model, such as the learning rate or the optimizer.

CQ: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The context does not provide information on the optimization techniques applied in the deep learning pipeline.

CQ: What are the regularization methods used to prevent overfitting  in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The context does not provide information on the regularization methods used to prevent overfitting in the deep learning pipeline. Therefore, I don't know the answer to this query based on the provided context.

CQ: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The Keras Deep learning library is used to build the deep learning model. It is used in CUDA-CuDNN GPU mode. Python code in Anaconda Python 2. 7. 11 running in Ubuntu Linux environment and the Linux version of Matlab 2014b are also used.

CQ: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The context mentions that the network is trained using a single TITAN-X GPU.

CQ: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The performance of the deep learning model is evaluated using the mean Dice coefficient loss function. The Dice coefficient is used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by: YXYX*2 Where, X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is the quotient of similarity and ranges between 0 and 1. It can be viewed as a similarity measure over sets. The loss function is just the minus of the Dice coefficient with the additions of a smoothing factor inserted in the denominator. The score in Table I is the mean of the Dice coefficients of images in the evaluation set.

CQ: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is segmentation. It is used to segment bird vocalizations from audio spectrograms without requiring manual, laborious tagging of thousands of audio spectrograms. The model is a modified version of [6] and is trained on spectrogram figures of bird recordings as well as recordings void of any bird activity and their corresponding binary masks. During training, the spectrogram is presented as input, and the mask is presented as output, whereas the network learns the mapping in-between them. The network is forced to learn, by an iterative procedure, the mapping of vocalizations to binary blobs and to suppress everything else.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...
